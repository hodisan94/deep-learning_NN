{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    # Loop through layers to initialize weights and biases\n",
    "    for l in range(1, L):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the function initialize_parameters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 784)\n",
      "(20, 1)\n",
      "(10, 5)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [784, 20, 7, 5, 10]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "print(parameters[\"W1\"].shape)  # Should print (20, 784)\n",
    "print(parameters[\"b1\"].shape)  # Should print (20, 1)\n",
    "print(parameters[\"W4\"].shape)  # Should print (10, 5)\n",
    "print(parameters[\"b4\"].shape)  # Should print (10, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Compute the linear part of a layer's forward propagation.\n",
    "    \n",
    "    Arguments:\n",
    "    A -- activations from previous layer\n",
    "    W -- weights matrix of current layer\n",
    "    b -- bias vector of current layer\n",
    "    \n",
    "    Returns:\n",
    "    Z -- linear component (W.A + b)\n",
    "    linear_cache -- tuple (A, W, b) for backpropagation\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    linear_cache = (A, W, b)\n",
    "    return Z, linear_cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the function linear_forward**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z shape: (4, 2)\n",
      "Z: [[ 0.48338097  0.15237448]\n",
      " [-1.33390458 -2.08407482]\n",
      " [ 0.7505205  -1.0779219 ]\n",
      " [-1.23465366 -1.76418096]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "A_prev = np.random.randn(3, 2)\n",
    "W = np.random.randn(4, 3)\n",
    "b = np.random.randn(4, 1)\n",
    "\n",
    "Z, cache = linear_forward(A_prev, W, b)\n",
    "print(\"Z shape:\", Z.shape)  # Should be (4, 2)\n",
    "print(\"Z:\", Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Implements the softmax activation function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of shape (number of classes, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    A -- output of softmax(Z), same shape as Z\n",
    "    activation_cache -- returns Z for use in backpropagation\n",
    "    \"\"\"\n",
    "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)  # for numerical stability\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    \n",
    "    activation_cache = Z\n",
    "    return A, activation_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implements the ReLU activation function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- output of relu(Z), same shape as Z\n",
    "    activation_cache -- returns Z for use in backpropagation\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU result:\n",
      " [[1 0 0]\n",
      " [2 0 3]]\n",
      "Softmax result:\n",
      " [[0.26894142 0.73105858 0.04742587]\n",
      " [0.73105858 0.26894142 0.95257413]]\n"
     ]
    }
   ],
   "source": [
    "Z = np.array([[1, -1, 0],\n",
    "              [2, -2, 3]])\n",
    "\n",
    "A_relu, cache_relu = relu(Z)\n",
    "A_softmax, cache_softmax = softmax(Z)\n",
    "\n",
    "print(\"ReLU result:\\n\", A_relu)\n",
    "print(\"Softmax result:\\n\", A_softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, B, activation):\n",
    "    \"\"\"\n",
    "    Implements forward propagation for LINEAR -> ACTIVATION.\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer\n",
    "    W -- weights matrix\n",
    "    B -- bias vector\n",
    "    activation -- the activation function to use: \"relu\" or \"softmax\"\n",
    "\n",
    "    Returns:\n",
    "    A -- output of the activation function\n",
    "    cache -- dictionary containing linear_cache and activation_cache\n",
    "    \"\"\"\n",
    "    Z, linear_cache = linear_forward(A_prev, W, B)\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        A, activation_cache = softmax(Z)\n",
    "    else:\n",
    "        raise ValueError(\"Activation must be 'relu' or 'softmax'\")\n",
    "\n",
    "    cache = {\n",
    "        \"linear_cache\": linear_cache,\n",
    "        \"activation_cache\": activation_cache\n",
    "    }\n",
    "\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU output:\n",
      " [[0.48338097 0.15237448]\n",
      " [0.         0.        ]\n",
      " [0.7505205  0.        ]\n",
      " [0.         0.        ]]\n",
      "Softmax output:\n",
      " [[0.37762821 0.64676528]\n",
      " [0.0613518  0.06909858]\n",
      " [0.49326653 0.18898867]\n",
      " [0.06775346 0.09514747]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "A_prev = np.random.randn(3, 2)\n",
    "W = np.random.randn(4, 3)\n",
    "B = np.random.randn(4, 1)\n",
    "\n",
    "A_relu, cache_relu = linear_activation_forward(A_prev, W, B, activation=\"relu\")\n",
    "print(\"ReLU output:\\n\", A_relu)\n",
    "\n",
    "A_softmax, cache_softmax = linear_activation_forward(A_prev, W, B, activation=\"softmax\")\n",
    "print(\"Softmax output:\\n\", A_softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_model_forward(X, parameters, use_batchnorm=False):\n",
    "    \"\"\"\n",
    "    Implements forward pass for [LINEAR->RELU]*(L-1) -> LINEAR -> SOFTMAX.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data\n",
    "    parameters -- dictionary containing W1...WL, b1...bL\n",
    "    use_batchnorm -- whether to apply batch normalization after activation\n",
    "\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches from each layer\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W = parameters[f\"W{l}\"]\n",
    "        b = parameters[f\"b{l}\"]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, activation=\"relu\")\n",
    "\n",
    "        if use_batchnorm:\n",
    "            A = apply_batchnorm(A)\n",
    "\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Final layer: LINEAR -> SOFTMAX\n",
    "    WL = parameters[f\"W{L}\"]\n",
    "    bL = parameters[f\"b{L}\"]\n",
    "    AL, cache = linear_activation_forward(A, WL, bL, activation=\"softmax\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implements the categorical cross-entropy loss function.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability predictions from softmax, shape (num_classes, m)\n",
    "    Y -- true labels (one-hot encoded), shape (num_classes, m)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost (scalar)\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Avoid log(0) by adding small epsilon\n",
    "    epsilon = 1e-15\n",
    "    AL = np.clip(AL, epsilon, 1 - epsilon)\n",
    "\n",
    "    cost = -np.sum(Y * np.log(AL)) / m\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.164252033486018\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([[1, 0], [0, 1]])  # true labels (2 classes, 2 samples)\n",
    "AL = np.array([[0.8, 0.1], [0.2, 0.9]])  # predictions\n",
    "\n",
    "cost = compute_cost(AL, Y)\n",
    "print(\"Cost:\", cost)  # Should be a small positive number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BatchNorm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_batchnorm(A, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Applies batch normalization to the activation values of a layer.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activation values of shape (layer size, number of examples)\n",
    "    epsilon -- small float to prevent division by zero\n",
    "\n",
    "    Returns:\n",
    "    NA -- normalized activations (same shape as A)\n",
    "    \"\"\"\n",
    "    mu = np.mean(A, axis=1, keepdims=True)\n",
    "    var = np.var(A, axis=1, keepdims=True)\n",
    "    NA = (A - mu) / np.sqrt(var + epsilon)\n",
    "    return NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization: mean = [-0.22077365 -0.34418034 -0.34990549  0.13905794  0.24136955]\n",
      "After normalization: mean = [ 4.44089210e-17 -2.22044605e-17 -8.46545056e-17  5.55111512e-18\n",
      "  2.22044605e-17]\n"
     ]
    }
   ],
   "source": [
    "A = np.random.randn(5, 10)\n",
    "NA = apply_batchnorm(A)\n",
    "print(\"Before normalization: mean =\", np.mean(A, axis=1))\n",
    "print(\"After normalization: mean =\", np.mean(NA, axis=1))  # should be ~0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implements the linear part of the backward propagation for a single layer.\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to Z, shape (n_l, m)\n",
    "    cache -- tuple of (A_prev, W, b) from the forward pass\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient wrt previous layer activations\n",
    "    dW -- Gradient wrt weights\n",
    "    db -- Gradient wrt biases\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient\n",
    "    cache -- tuple of (linear_cache, activation_cache)\n",
    "    activation -- \"relu\" or \"softmax\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev, dW, db -- gradients for previous layer, weights, and biases\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        Z = activation_cache\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "\n",
    "    elif activation == \"softmax\":\n",
    "        # Softmax + Cross-Entropy: dA is already (AL - Y)\n",
    "        dZ = dA\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation. Use 'relu' or 'softmax'.\")\n",
    "\n",
    "    return linear_backward(dZ, linear_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Implements backward propagation for a ReLU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, same shape as Z\n",
    "    activation_cache -- Z (from forward propagation)\n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Implements backward propagation for a softmax unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient (assumed to be AL - Y), shape (classes, m)\n",
    "    activation_cache -- Z (pre-activation values from forward propagation)\n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of the cost with respect to Z (same as dA in this case)\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)  # for stability\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    softmax = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "\n",
    "    # In most practical cases, dA is AL - Y, so:\n",
    "    dZ = dA  # already the correct gradient for softmax + cross-entropy\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implements the full backward propagation for [LINEAR->RELU]*(L-1) -> LINEAR->SOFTMAX\n",
    "\n",
    "    Arguments:\n",
    "    AL -- softmax output from forward propagation, shape (classes, m)\n",
    "    Y -- true labels (one-hot), same shape as AL\n",
    "    caches -- list of caches from each layer (from forward propagation)\n",
    "\n",
    "    Returns:\n",
    "    grads -- dictionary of gradients: dA, dW, db for each layer\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "\n",
    "    # 1 Output layer (Softmax)\n",
    "    current_cache = caches[-1]\n",
    "    dZL = AL - Y  # ∂L/∂Z for softmax + cross-entropy\n",
    "    dA_prev, dW, db = linear_backward(dZL, current_cache[\"linear_cache\"])\n",
    "    \n",
    "    grads[f\"dA{L}\"] = dA_prev\n",
    "    grads[f\"dW{L}\"] = dW\n",
    "    grads[f\"db{L}\"] = db\n",
    "\n",
    "    # 2 Hidden layers (ReLU)\n",
    "    for l in reversed(range(L - 1)):\n",
    "        current_cache = caches[l]\n",
    "        dA = grads[f\"dA{l+2}\"]\n",
    "        dZ = relu_backward(dA, current_cache[\"activation_cache\"])\n",
    "        dA_prev, dW, db = linear_backward(dZ, current_cache[\"linear_cache\"])\n",
    "\n",
    "        grads[f\"dA{l+1}\"] = dA_prev\n",
    "        grads[f\"dW{l+1}\"] = dW\n",
    "        grads[f\"db{l+1}\"] = db\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates weights and biases using gradient descent.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- dictionary containing W1...WL and b1...bL\n",
    "    grads -- dictionary containing dW1...dWL and db1...dbL\n",
    "    learning_rate -- scalar learning rate\n",
    "\n",
    "    Returns:\n",
    "    parameters -- updated dictionary with new weights and biases\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2  # number of layers\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f\"W{l}\"] -= learning_rate * grads[f\"dW{l}\"]\n",
    "        parameters[f\"b{l}\"] -= learning_rate * grads[f\"db{l}\"]\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests for Part 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_backward:\n",
      "dA_prev:\n",
      " [[-0.87596241  3.15776973]\n",
      " [-2.24081818  6.23229549]\n",
      " [-1.06441566  1.11304416]] \n",
      "dW:\n",
      " [[-0.27749007  1.21567825 -0.14639757]\n",
      " [ 0.0697697  -0.80983366  0.12540482]\n",
      " [ 0.2886844  -0.66331421  0.04662148]\n",
      " [ 0.18734896 -1.42467878  0.20496254]] \n",
      "db:\n",
      " [[ 0.62528579]\n",
      " [-0.53560408]\n",
      " [-0.19914937]\n",
      " [-0.87540326]]\n",
      "-------------------------\n",
      "\n",
      "\n",
      "relu_backward:\n",
      "dZ:\n",
      " [[ 0.0675282  -1.42474819]\n",
      " [ 0.          0.        ]\n",
      " [-1.15099358  0.        ]\n",
      " [ 0.          0.        ]]\n",
      "-------------------------\n",
      "\n",
      "\n",
      "softmax_backward:\n",
      "dZ:\n",
      " [[-0.2   0.1 ]\n",
      " [ 0.1  -0.2 ]\n",
      " [ 0.05  0.05]\n",
      " [ 0.05  0.05]]\n",
      "-------------------------\n",
      "\n",
      "\n",
      "linear_activation_backward (ReLU):\n",
      "dA_prev:\n",
      " [[-0.17185561 -2.24998059]\n",
      " [ 2.25399676 -1.09340124]\n",
      " [ 1.95366658  0.66888278]] \n",
      "dW:\n",
      " [[ 0.11526701 -1.06309839  0.15888712]\n",
      " [ 0.          0.          0.        ]\n",
      " [-0.2858574  -0.37274267  0.13475452]\n",
      " [ 0.          0.          0.        ]] \n",
      "db:\n",
      " [[-0.67860999]\n",
      " [ 0.        ]\n",
      " [-0.57549679]\n",
      " [ 0.        ]]\n",
      "-------------------------\n",
      "\n",
      "\n",
      "l_model_backward:\n",
      "dW1:\n",
      " [[ 0.07918554 -0.04402687  0.00689487]\n",
      " [-0.02175084 -0.01893417  0.0332684 ]\n",
      " [ 0.02934355  0.02554365 -0.04488162]\n",
      " [ 0.14972284 -0.07319003  0.00164142]] \n",
      "db1:\n",
      " [[-0.05961931]\n",
      " [-0.11048817]\n",
      " [ 0.14905704]\n",
      " [-0.07161341]]\n",
      "-------------------------\n",
      "\n",
      "\n",
      "update_parameters:\n",
      "Updated W1:\n",
      " [[-1.48644054 -0.71544152 -0.46132826]\n",
      " [ 1.05929731  0.34551171 -1.766367  ]\n",
      " [ 0.32114961 -0.38763665 -0.67243384]\n",
      " [ 0.596704    1.03831853  0.93111598]] \n",
      "Updated b1:\n",
      " [[-0.83325559]\n",
      " [-0.29816356]\n",
      " [ 0.31635773]\n",
      " [ 0.98270647]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "A_prev = np.random.randn(3, 2)\n",
    "W = np.random.randn(4, 3)\n",
    "b = np.random.randn(4, 1)\n",
    "Z = np.dot(W, A_prev) + b\n",
    "\n",
    "# simulate dA for testing\n",
    "dA = np.random.randn(4, 2)\n",
    "Y = np.array([[1, 0], [0, 1], [0, 0], [0, 0]])  # one-hot labels\n",
    "AL = np.array([[0.8, 0.1], [0.1, 0.8], [0.05, 0.05], [0.05, 0.05]])\n",
    "\n",
    "linear_cache = (A_prev, W, b)\n",
    "activation_cache = Z\n",
    "cache = {\"linear_cache\": linear_cache, \"activation_cache\": activation_cache}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dZ = np.random.randn(4, 2)\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print(\"linear_backward:\\ndA_prev:\\n\", dA_prev, \"\\ndW:\\n\", dW, \"\\ndb:\\n\", db)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print ('\\n')\n",
    "\n",
    "dZ_relu = relu_backward(dA, activation_cache)\n",
    "print(\"relu_backward:\\ndZ:\\n\", dZ_relu)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print ('\\n')\n",
    "\n",
    "dZ_softmax = softmax_backward(AL - Y, activation_cache)\n",
    "print(\"softmax_backward:\\ndZ:\\n\", dZ_softmax)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print ('\\n')\n",
    "\n",
    "dA_prev_la, dW_la, db_la = linear_activation_backward(dA, (linear_cache, activation_cache), \"relu\")\n",
    "print(\"linear_activation_backward (ReLU):\\ndA_prev:\\n\", dA_prev_la, \"\\ndW:\\n\", dW_la, \"\\ndb:\\n\", db_la)\n",
    "\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print ('\\n')\n",
    "\n",
    "# simulate forward caches\n",
    "# Layer 1 cache (3 -> 4)\n",
    "A1 = np.random.randn(3, 2)\n",
    "W1 = np.random.randn(4, 3)\n",
    "b1 = np.random.randn(4, 1)\n",
    "Z1 = np.dot(W1, A1) + b1\n",
    "cache1 = {\n",
    "    \"linear_cache\": (A1, W1, b1),\n",
    "    \"activation_cache\": Z1\n",
    "}\n",
    "\n",
    "# Layer 2 cache (4 -> 2)\n",
    "A2 = np.random.randn(4, 2)\n",
    "W2 = np.random.randn(2, 4)\n",
    "b2 = np.random.randn(2, 1)\n",
    "Z2 = np.dot(W2, A2) + b2\n",
    "cache2 = {\n",
    "    \"linear_cache\": (A2, W2, b2),\n",
    "    \"activation_cache\": Z2\n",
    "}\n",
    "\n",
    "# Simulate softmax output and labels\n",
    "AL = np.array([[0.7, 0.1], [0.3, 0.9]])\n",
    "Y = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "caches = [cache1, cache2] \n",
    "grads = l_model_backward(AL, Y, caches)\n",
    "print(\"l_model_backward:\\ndW1:\\n\", grads[\"dW1\"], \"\\ndb1:\\n\", grads[\"db1\"])\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print ('\\n')\n",
    "\n",
    "parameters = {\n",
    "    \"W1\": np.copy(W1),\n",
    "    \"b1\": np.copy(b1),\n",
    "    \"W2\": np.copy(W2),\n",
    "    \"b2\": np.copy(b2),\n",
    "}\n",
    "\n",
    "grads = {\n",
    "    \"dW1\": grads[\"dW1\"],\n",
    "    \"db1\": grads[\"db1\"],\n",
    "    \"dW2\": grads[\"dW2\"],\n",
    "    \"db2\": grads[\"db2\"],\n",
    "}\n",
    "updated = update_parameters(parameters, grads, 0.1)\n",
    "print(\"update_parameters:\\nUpdated W1:\\n\", updated[\"W1\"], \"\\nUpdated b1:\\n\", updated[\"b1\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    # Loop through layers to initialize weights and biases\n",
    "    for l in range(1, L):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the function initialize_parameters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 784)\n",
      "(20, 1)\n",
      "(10, 5)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [784, 20, 7, 5, 10]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "print(parameters[\"W1\"].shape)  # Should print (20, 784)\n",
    "print(parameters[\"b1\"].shape)  # Should print (20, 1)\n",
    "print(parameters[\"W4\"].shape)  # Should print (10, 5)\n",
    "print(parameters[\"b4\"].shape)  # Should print (10, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Compute the linear part of a layer's forward propagation.\n",
    "    \n",
    "    Arguments:\n",
    "    A -- activations from previous layer\n",
    "    W -- weights matrix of current layer\n",
    "    b -- bias vector of current layer\n",
    "    \n",
    "    Returns:\n",
    "    Z -- linear component (W.A + b)\n",
    "    linear_cache -- tuple (A, W, b) for backpropagation\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    linear_cache = (A, W, b)\n",
    "    return Z, linear_cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the function linear_forward**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z shape: (4, 2)\n",
      "Z: [[ 0.48338097  0.15237448]\n",
      " [-1.33390458 -2.08407482]\n",
      " [ 0.7505205  -1.0779219 ]\n",
      " [-1.23465366 -1.76418096]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "A_prev = np.random.randn(3, 2)\n",
    "W = np.random.randn(4, 3)\n",
    "b = np.random.randn(4, 1)\n",
    "\n",
    "Z, cache = linear_forward(A_prev, W, b)\n",
    "print(\"Z shape:\", Z.shape)  # Should be (4, 2)\n",
    "print(\"Z:\", Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Implements the softmax activation function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of shape (number of classes, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    A -- output of softmax(Z), same shape as Z\n",
    "    activation_cache -- returns Z for use in backpropagation\n",
    "    \"\"\"\n",
    "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)  # for numerical stability\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    \n",
    "    activation_cache = Z\n",
    "    return A, activation_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implements the ReLU activation function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- output of relu(Z), same shape as Z\n",
    "    activation_cache -- returns Z for use in backpropagation\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU result:\n",
      " [[1 0 0]\n",
      " [2 0 3]]\n",
      "Softmax result:\n",
      " [[0.26894142 0.73105858 0.04742587]\n",
      " [0.73105858 0.26894142 0.95257413]]\n"
     ]
    }
   ],
   "source": [
    "Z = np.array([[1, -1, 0],\n",
    "              [2, -2, 3]])\n",
    "\n",
    "A_relu, cache_relu = relu(Z)\n",
    "A_softmax, cache_softmax = softmax(Z)\n",
    "\n",
    "print(\"ReLU result:\\n\", A_relu)\n",
    "print(\"Softmax result:\\n\", A_softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, B, activation):\n",
    "    \"\"\"\n",
    "    Implements forward propagation for LINEAR -> ACTIVATION.\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer\n",
    "    W -- weights matrix\n",
    "    B -- bias vector\n",
    "    activation -- the activation function to use: \"relu\" or \"softmax\"\n",
    "\n",
    "    Returns:\n",
    "    A -- output of the activation function\n",
    "    cache -- dictionary containing linear_cache and activation_cache\n",
    "    \"\"\"\n",
    "    Z, linear_cache = linear_forward(A_prev, W, B)\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        A, activation_cache = softmax(Z)\n",
    "    else:\n",
    "        raise ValueError(\"Activation must be 'relu' or 'softmax'\")\n",
    "\n",
    "    cache = {\n",
    "        \"linear_cache\": linear_cache,\n",
    "        \"activation_cache\": activation_cache\n",
    "    }\n",
    "\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU output:\n",
      " [[0.48338097 0.15237448]\n",
      " [0.         0.        ]\n",
      " [0.7505205  0.        ]\n",
      " [0.         0.        ]]\n",
      "Softmax output:\n",
      " [[0.37762821 0.64676528]\n",
      " [0.0613518  0.06909858]\n",
      " [0.49326653 0.18898867]\n",
      " [0.06775346 0.09514747]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "A_prev = np.random.randn(3, 2)\n",
    "W = np.random.randn(4, 3)\n",
    "B = np.random.randn(4, 1)\n",
    "\n",
    "A_relu, cache_relu = linear_activation_forward(A_prev, W, B, activation=\"relu\")\n",
    "print(\"ReLU output:\\n\", A_relu)\n",
    "\n",
    "A_softmax, cache_softmax = linear_activation_forward(A_prev, W, B, activation=\"softmax\")\n",
    "print(\"Softmax output:\\n\", A_softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_model_forward(X, parameters, use_batchnorm=False):\n",
    "    \"\"\"\n",
    "    Implements forward pass for [LINEAR->RELU]*(L-1) -> LINEAR -> SOFTMAX.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data\n",
    "    parameters -- dictionary containing W1...WL, b1...bL\n",
    "    use_batchnorm -- whether to apply batch normalization after activation\n",
    "\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches from each layer\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W = parameters[f\"W{l}\"]\n",
    "        b = parameters[f\"b{l}\"]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, activation=\"relu\")\n",
    "\n",
    "        if use_batchnorm:\n",
    "            A = apply_batchnorm(A)\n",
    "\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Final layer: LINEAR -> SOFTMAX\n",
    "    WL = parameters[f\"W{L}\"]\n",
    "    bL = parameters[f\"b{L}\"]\n",
    "    AL, cache = linear_activation_forward(A, WL, bL, activation=\"softmax\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implements the categorical cross-entropy loss function.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability predictions from softmax, shape (num_classes, m)\n",
    "    Y -- true labels (one-hot encoded), shape (num_classes, m)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost (scalar)\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Avoid log(0) by adding small epsilon\n",
    "    epsilon = 1e-15\n",
    "    AL = np.clip(AL, epsilon, 1 - epsilon)\n",
    "\n",
    "    cost = -np.sum(Y * np.log(AL)) / m\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.164252033486018\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([[1, 0], [0, 1]])  # true labels (2 classes, 2 samples)\n",
    "AL = np.array([[0.8, 0.1], [0.2, 0.9]])  # predictions\n",
    "\n",
    "cost = compute_cost(AL, Y)\n",
    "print(\"Cost:\", cost)  # Should be a small positive number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BatchNorm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_batchnorm(A, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Applies batch normalization to the activation values of a layer.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activation values of shape (layer size, number of examples)\n",
    "    epsilon -- small float to prevent division by zero\n",
    "\n",
    "    Returns:\n",
    "    NA -- normalized activations (same shape as A)\n",
    "    \"\"\"\n",
    "    mu = np.mean(A, axis=1, keepdims=True)\n",
    "    var = np.var(A, axis=1, keepdims=True)\n",
    "    NA = (A - mu) / np.sqrt(var + epsilon)\n",
    "    return NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization: mean = [-0.22077365 -0.34418034 -0.34990549  0.13905794  0.24136955]\n",
      "After normalization: mean = [ 4.44089210e-17 -2.22044605e-17 -8.46545056e-17  5.55111512e-18\n",
      "  2.22044605e-17]\n"
     ]
    }
   ],
   "source": [
    "A = np.random.randn(5, 10)\n",
    "NA = apply_batchnorm(A)\n",
    "print(\"Before normalization: mean =\", np.mean(A, axis=1))\n",
    "print(\"After normalization: mean =\", np.mean(NA, axis=1))  # should be ~0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

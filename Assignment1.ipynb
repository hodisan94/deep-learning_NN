{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    # Loop through layers to initialize weights and biases\n",
    "    for l in range(1, L):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the function initialize_parameters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 784)\n",
      "(20, 1)\n",
      "(10, 5)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [784, 20, 7, 5, 10]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "print(parameters[\"W1\"].shape)  # Should print (20, 784)\n",
    "print(parameters[\"b1\"].shape)  # Should print (20, 1)\n",
    "print(parameters[\"W4\"].shape)  # Should print (10, 5)\n",
    "print(parameters[\"b4\"].shape)  # Should print (10, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Compute the linear part of a layer's forward propagation.\n",
    "    \n",
    "    Arguments:\n",
    "    A -- activations from previous layer\n",
    "    W -- weights matrix of current layer\n",
    "    b -- bias vector of current layer\n",
    "    \n",
    "    Returns:\n",
    "    Z -- linear component (W.A + b)\n",
    "    linear_cache -- tuple (A, W, b) for backpropagation\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    linear_cache = (A, W, b)\n",
    "    return Z, linear_cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the function linear_forward**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z shape: (4, 2)\n",
      "Z: [[ 0.48338097  0.15237448]\n",
      " [-1.33390458 -2.08407482]\n",
      " [ 0.7505205  -1.0779219 ]\n",
      " [-1.23465366 -1.76418096]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "A_prev = np.random.randn(3, 2)\n",
    "W = np.random.randn(4, 3)\n",
    "b = np.random.randn(4, 1)\n",
    "\n",
    "Z, cache = linear_forward(A_prev, W, b)\n",
    "print(\"Z shape:\", Z.shape)  # Should be (4, 2)\n",
    "print(\"Z:\", Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Implements the softmax activation function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of shape (number of classes, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    A -- output of softmax(Z), same shape as Z\n",
    "    activation_cache -- returns Z for use in backpropagation\n",
    "    \"\"\"\n",
    "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)  # for numerical stability\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    \n",
    "    activation_cache = Z\n",
    "    return A, activation_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implements the ReLU activation function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- output of relu(Z), same shape as Z\n",
    "    activation_cache -- returns Z for use in backpropagation\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU result:\n",
      " [[1 0 0]\n",
      " [2 0 3]]\n",
      "Softmax result:\n",
      " [[0.26894142 0.73105858 0.04742587]\n",
      " [0.73105858 0.26894142 0.95257413]]\n"
     ]
    }
   ],
   "source": [
    "Z = np.array([[1, -1, 0],\n",
    "              [2, -2, 3]])\n",
    "\n",
    "A_relu, cache_relu = relu(Z)\n",
    "A_softmax, cache_softmax = softmax(Z)\n",
    "\n",
    "print(\"ReLU result:\\n\", A_relu)\n",
    "print(\"Softmax result:\\n\", A_softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, B, activation):\n",
    "    \"\"\"\n",
    "    Implements forward propagation for LINEAR -> ACTIVATION.\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer\n",
    "    W -- weights matrix\n",
    "    B -- bias vector\n",
    "    activation -- the activation function to use: \"relu\" or \"softmax\"\n",
    "\n",
    "    Returns:\n",
    "    A -- output of the activation function\n",
    "    cache -- dictionary containing linear_cache and activation_cache\n",
    "    \"\"\"\n",
    "    Z, linear_cache = linear_forward(A_prev, W, B)\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        A, activation_cache = softmax(Z)\n",
    "    else:\n",
    "        raise ValueError(\"Activation must be 'relu' or 'softmax'\")\n",
    "\n",
    "    cache = {\n",
    "        \"linear_cache\": linear_cache,\n",
    "        \"activation_cache\": activation_cache\n",
    "    }\n",
    "\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU output:\n",
      " [[0.48338097 0.15237448]\n",
      " [0.         0.        ]\n",
      " [0.7505205  0.        ]\n",
      " [0.         0.        ]]\n",
      "Softmax output:\n",
      " [[0.37762821 0.64676528]\n",
      " [0.0613518  0.06909858]\n",
      " [0.49326653 0.18898867]\n",
      " [0.06775346 0.09514747]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "A_prev = np.random.randn(3, 2)\n",
    "W = np.random.randn(4, 3)\n",
    "B = np.random.randn(4, 1)\n",
    "\n",
    "A_relu, cache_relu = linear_activation_forward(A_prev, W, B, activation=\"relu\")\n",
    "print(\"ReLU output:\\n\", A_relu)\n",
    "\n",
    "A_softmax, cache_softmax = linear_activation_forward(A_prev, W, B, activation=\"softmax\")\n",
    "print(\"Softmax output:\\n\", A_softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_model_forward(X, parameters, use_batchnorm=False):\n",
    "    \"\"\"\n",
    "    Implements forward pass for [LINEAR->RELU]*(L-1) -> LINEAR -> SOFTMAX.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data\n",
    "    parameters -- dictionary containing W1...WL, b1...bL\n",
    "    use_batchnorm -- whether to apply batch normalization after activation\n",
    "\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches from each layer\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W = parameters[f\"W{l}\"]\n",
    "        b = parameters[f\"b{l}\"]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, activation=\"relu\")\n",
    "\n",
    "        if use_batchnorm:\n",
    "            A = apply_batchnorm(A)\n",
    "\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Final layer: LINEAR -> SOFTMAX\n",
    "    WL = parameters[f\"W{L}\"]\n",
    "    bL = parameters[f\"b{L}\"]\n",
    "    AL, cache = linear_activation_forward(A, WL, bL, activation=\"softmax\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implements the categorical cross-entropy loss function.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability predictions from softmax, shape (num_classes, m)\n",
    "    Y -- true labels (one-hot encoded), shape (num_classes, m)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost (scalar)\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Avoid log(0) by adding small epsilon\n",
    "    epsilon = 1e-15\n",
    "    AL = np.clip(AL, epsilon, 1 - epsilon)\n",
    "\n",
    "    cost = -np.sum(Y * np.log(AL)) / m\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.164252033486018\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([[1, 0], [0, 1]])  # true labels (2 classes, 2 samples)\n",
    "AL = np.array([[0.8, 0.1], [0.2, 0.9]])  # predictions\n",
    "\n",
    "cost = compute_cost(AL, Y)\n",
    "print(\"Cost:\", cost)  # Should be a small positive number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BatchNorm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_batchnorm(A, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Applies batch normalization to the activation values of a layer.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activation values of shape (layer size, number of examples)\n",
    "    epsilon -- small float to prevent division by zero\n",
    "\n",
    "    Returns:\n",
    "    NA -- normalized activations (same shape as A)\n",
    "    \"\"\"\n",
    "    mu = np.mean(A, axis=1, keepdims=True)\n",
    "    var = np.var(A, axis=1, keepdims=True)\n",
    "    NA = (A - mu) / np.sqrt(var + epsilon)\n",
    "    return NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization: mean = [-0.22077365 -0.34418034 -0.34990549  0.13905794  0.24136955]\n",
      "After normalization: mean = [ 4.44089210e-17 -2.22044605e-17 -8.46545056e-17  5.55111512e-18\n",
      "  2.22044605e-17]\n"
     ]
    }
   ],
   "source": [
    "A = np.random.randn(5, 10)\n",
    "NA = apply_batchnorm(A)\n",
    "print(\"Before normalization: mean =\", np.mean(A, axis=1))\n",
    "print(\"After normalization: mean =\", np.mean(NA, axis=1))  # should be ~0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implements the linear part of the backward propagation for a single layer.\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to Z, shape (n_l, m)\n",
    "    cache -- tuple of (A_prev, W, b) from the forward pass\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient wrt previous layer activations\n",
    "    dW -- Gradient wrt weights\n",
    "    db -- Gradient wrt biases\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient\n",
    "    cache -- tuple of (linear_cache, activation_cache)\n",
    "    activation -- \"relu\" or \"softmax\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev, dW, db -- gradients for previous layer, weights, and biases\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        Z = activation_cache\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "\n",
    "    elif activation == \"softmax\":\n",
    "        # Softmax + Cross-Entropy: dA is already (AL - Y)\n",
    "        dZ = dA\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation. Use 'relu' or 'softmax'.\")\n",
    "\n",
    "    return linear_backward(dZ, linear_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Implements backward propagation for a ReLU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, same shape as Z\n",
    "    activation_cache -- Z (from forward propagation)\n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Implements backward propagation for a softmax unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient (assumed to be AL - Y), shape (classes, m)\n",
    "    activation_cache -- Z (pre-activation values from forward propagation)\n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of the cost with respect to Z (same as dA in this case)\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)  # for stability\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    softmax = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "\n",
    "    # In most practical cases, dA is AL - Y, so:\n",
    "    dZ = dA  # already the correct gradient for softmax + cross-entropy\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implements the full backward propagation for [LINEAR->RELU]*(L-1) -> LINEAR->SOFTMAX\n",
    "\n",
    "    Arguments:\n",
    "    AL -- softmax output from forward propagation, shape (classes, m)\n",
    "    Y -- true labels (one-hot), same shape as AL\n",
    "    caches -- list of caches from each layer (from forward propagation)\n",
    "\n",
    "    Returns:\n",
    "    grads -- dictionary of gradients: dA, dW, db for each layer\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "\n",
    "    # 1 Output layer (Softmax)\n",
    "    current_cache = caches[-1]\n",
    "    dZL = AL - Y  # ∂L/∂Z for softmax + cross-entropy\n",
    "    dA_prev, dW, db = linear_backward(dZL, current_cache[\"linear_cache\"])\n",
    "    \n",
    "    grads[f\"dA{L}\"] = dA_prev\n",
    "    grads[f\"dW{L}\"] = dW\n",
    "    grads[f\"db{L}\"] = db\n",
    "\n",
    "    # 2 Hidden layers (ReLU)\n",
    "    for l in reversed(range(L - 1)):\n",
    "        current_cache = caches[l]\n",
    "        dA = grads[f\"dA{l+2}\"]\n",
    "        dZ = relu_backward(dA, current_cache[\"activation_cache\"])\n",
    "        dA_prev, dW, db = linear_backward(dZ, current_cache[\"linear_cache\"])\n",
    "\n",
    "        grads[f\"dA{l+1}\"] = dA_prev\n",
    "        grads[f\"dW{l+1}\"] = dW\n",
    "        grads[f\"db{l+1}\"] = db\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates weights and biases using gradient descent.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- dictionary containing W1...WL and b1...bL\n",
    "    grads -- dictionary containing dW1...dWL and db1...dbL\n",
    "    learning_rate -- scalar learning rate\n",
    "\n",
    "    Returns:\n",
    "    parameters -- updated dictionary with new weights and biases\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2  # number of layers\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f\"W{l}\"] -= learning_rate * grads[f\"dW{l}\"]\n",
    "        parameters[f\"b{l}\"] -= learning_rate * grads[f\"db{l}\"]\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests for Part 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_backward:\n",
      "dA_prev:\n",
      " [[-0.87596241  3.15776973]\n",
      " [-2.24081818  6.23229549]\n",
      " [-1.06441566  1.11304416]] \n",
      "dW:\n",
      " [[-0.27749007  1.21567825 -0.14639757]\n",
      " [ 0.0697697  -0.80983366  0.12540482]\n",
      " [ 0.2886844  -0.66331421  0.04662148]\n",
      " [ 0.18734896 -1.42467878  0.20496254]] \n",
      "db:\n",
      " [[ 0.62528579]\n",
      " [-0.53560408]\n",
      " [-0.19914937]\n",
      " [-0.87540326]]\n",
      "-------------------------\n",
      "\n",
      "\n",
      "relu_backward:\n",
      "dZ:\n",
      " [[ 0.0675282  -1.42474819]\n",
      " [ 0.          0.        ]\n",
      " [-1.15099358  0.        ]\n",
      " [ 0.          0.        ]]\n",
      "-------------------------\n",
      "\n",
      "\n",
      "softmax_backward:\n",
      "dZ:\n",
      " [[-0.2   0.1 ]\n",
      " [ 0.1  -0.2 ]\n",
      " [ 0.05  0.05]\n",
      " [ 0.05  0.05]]\n",
      "-------------------------\n",
      "\n",
      "\n",
      "linear_activation_backward (ReLU):\n",
      "dA_prev:\n",
      " [[-0.17185561 -2.24998059]\n",
      " [ 2.25399676 -1.09340124]\n",
      " [ 1.95366658  0.66888278]] \n",
      "dW:\n",
      " [[ 0.11526701 -1.06309839  0.15888712]\n",
      " [ 0.          0.          0.        ]\n",
      " [-0.2858574  -0.37274267  0.13475452]\n",
      " [ 0.          0.          0.        ]] \n",
      "db:\n",
      " [[-0.67860999]\n",
      " [ 0.        ]\n",
      " [-0.57549679]\n",
      " [ 0.        ]]\n",
      "-------------------------\n",
      "\n",
      "\n",
      "l_model_backward:\n",
      "dW1:\n",
      " [[ 0.07918554 -0.04402687  0.00689487]\n",
      " [-0.02175084 -0.01893417  0.0332684 ]\n",
      " [ 0.02934355  0.02554365 -0.04488162]\n",
      " [ 0.14972284 -0.07319003  0.00164142]] \n",
      "db1:\n",
      " [[-0.05961931]\n",
      " [-0.11048817]\n",
      " [ 0.14905704]\n",
      " [-0.07161341]]\n",
      "-------------------------\n",
      "\n",
      "\n",
      "update_parameters:\n",
      "Updated W1:\n",
      " [[-1.48644054 -0.71544152 -0.46132826]\n",
      " [ 1.05929731  0.34551171 -1.766367  ]\n",
      " [ 0.32114961 -0.38763665 -0.67243384]\n",
      " [ 0.596704    1.03831853  0.93111598]] \n",
      "Updated b1:\n",
      " [[-0.83325559]\n",
      " [-0.29816356]\n",
      " [ 0.31635773]\n",
      " [ 0.98270647]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "A_prev = np.random.randn(3, 2)\n",
    "W = np.random.randn(4, 3)\n",
    "b = np.random.randn(4, 1)\n",
    "Z = np.dot(W, A_prev) + b\n",
    "\n",
    "# simulate dA for testing\n",
    "dA = np.random.randn(4, 2)\n",
    "Y = np.array([[1, 0], [0, 1], [0, 0], [0, 0]])  # one-hot labels\n",
    "AL = np.array([[0.8, 0.1], [0.1, 0.8], [0.05, 0.05], [0.05, 0.05]])\n",
    "\n",
    "linear_cache = (A_prev, W, b)\n",
    "activation_cache = Z\n",
    "cache = {\"linear_cache\": linear_cache, \"activation_cache\": activation_cache}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dZ = np.random.randn(4, 2)\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print(\"linear_backward:\\ndA_prev:\\n\", dA_prev, \"\\ndW:\\n\", dW, \"\\ndb:\\n\", db)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print ('\\n')\n",
    "\n",
    "dZ_relu = relu_backward(dA, activation_cache)\n",
    "print(\"relu_backward:\\ndZ:\\n\", dZ_relu)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print ('\\n')\n",
    "\n",
    "dZ_softmax = softmax_backward(AL - Y, activation_cache)\n",
    "print(\"softmax_backward:\\ndZ:\\n\", dZ_softmax)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print ('\\n')\n",
    "\n",
    "dA_prev_la, dW_la, db_la = linear_activation_backward(dA, (linear_cache, activation_cache), \"relu\")\n",
    "print(\"linear_activation_backward (ReLU):\\ndA_prev:\\n\", dA_prev_la, \"\\ndW:\\n\", dW_la, \"\\ndb:\\n\", db_la)\n",
    "\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print ('\\n')\n",
    "\n",
    "# simulate forward caches\n",
    "# Layer 1 cache (3 -> 4)\n",
    "A1 = np.random.randn(3, 2)\n",
    "W1 = np.random.randn(4, 3)\n",
    "b1 = np.random.randn(4, 1)\n",
    "Z1 = np.dot(W1, A1) + b1\n",
    "cache1 = {\n",
    "    \"linear_cache\": (A1, W1, b1),\n",
    "    \"activation_cache\": Z1\n",
    "}\n",
    "\n",
    "# Layer 2 cache (4 -> 2)\n",
    "A2 = np.random.randn(4, 2)\n",
    "W2 = np.random.randn(2, 4)\n",
    "b2 = np.random.randn(2, 1)\n",
    "Z2 = np.dot(W2, A2) + b2\n",
    "cache2 = {\n",
    "    \"linear_cache\": (A2, W2, b2),\n",
    "    \"activation_cache\": Z2\n",
    "}\n",
    "\n",
    "# Simulate softmax output and labels\n",
    "AL = np.array([[0.7, 0.1], [0.3, 0.9]])\n",
    "Y = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "caches = [cache1, cache2] \n",
    "grads = l_model_backward(AL, Y, caches)\n",
    "print(\"l_model_backward:\\ndW1:\\n\", grads[\"dW1\"], \"\\ndb1:\\n\", grads[\"db1\"])\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print ('\\n')\n",
    "\n",
    "parameters = {\n",
    "    \"W1\": np.copy(W1),\n",
    "    \"b1\": np.copy(b1),\n",
    "    \"W2\": np.copy(W2),\n",
    "    \"b2\": np.copy(b2),\n",
    "}\n",
    "\n",
    "grads = {\n",
    "    \"dW1\": grads[\"dW1\"],\n",
    "    \"db1\": grads[\"db1\"],\n",
    "    \"dW2\": grads[\"dW2\"],\n",
    "    \"db2\": grads[\"db2\"],\n",
    "}\n",
    "updated = update_parameters(parameters, grads, 0.1)\n",
    "print(\"update_parameters:\\nUpdated W1:\\n\", updated[\"W1\"], \"\\nUpdated b1:\\n\", updated[\"b1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_layer_model(X, Y, layer_dims, learning_rate=0.009, num_iterations=3000, use_batchnorm=False, print_cost=True):\n",
    "    \"\"\"\n",
    "    Implements an L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, shape (input size, number of examples)\n",
    "    Y -- true labels (one-hot), shape (num_classes, number of examples)\n",
    "    layer_dims -- list of layer sizes, including input and output\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    num_iterations -- number of training iterations\n",
    "    use_batchnorm -- whether to use batch normalization\n",
    "    print_cost -- if True, prints cost every 100 iterations\n",
    "\n",
    "    Returns:\n",
    "    parameters -- learned weights and biases\n",
    "    costs -- list of cost values (every 100 iterations)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    costs = []\n",
    "\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "\n",
    "    for i in range(1, num_iterations + 1):\n",
    "        # Forward propagation\n",
    "        AL, caches = l_model_forward(X, parameters, use_batchnorm)\n",
    "\n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        # Backward propagation\n",
    "        grads = l_model_backward(AL, Y, caches)\n",
    "\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Record and print cost\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                print(f\"Cost after iteration {i}: {cost:.6f}\")\n",
    "\n",
    "    return parameters, costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    Predicts labels for given input X and compares to true labels Y.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data of shape (input size, number of examples)\n",
    "    Y -- true one-hot encoded labels (num_classes, number of examples)\n",
    "    parameters -- trained parameters dictionary\n",
    "\n",
    "    Returns:\n",
    "    accuracy -- prediction accuracy (0 to 1)\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    AL, _ = l_model_forward(X, parameters, use_batchnorm=False)  # don't apply BN during inference\n",
    "\n",
    "    # Predicted class = index of max probability\n",
    "    predictions = np.argmax(AL, axis=0)\n",
    "    true_labels = np.argmax(Y, axis=0)\n",
    "\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests for part 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 100: 1.093585\n",
      "Cost after iteration 200: 1.093245\n",
      "Cost after iteration 300: 1.092528\n",
      "Cost after iteration 400: 1.084721\n",
      "Cost after iteration 500: 0.617593\n",
      "\n",
      "✅ Fake training accuracy: 70.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Sanity Check for l_layer_model + predict ===\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fake data: 784 features, 3 classes, 50 examples\n",
    "X_fake = np.random.randn(784, 50)\n",
    "Y_labels = np.random.randint(0, 3, 50)\n",
    "\n",
    "# One-hot encode labels\n",
    "Y_fake = np.zeros((3, 50))\n",
    "Y_fake[Y_labels, np.arange(50)] = 1\n",
    "\n",
    "# Define simple network structure\n",
    "layer_dims = [784, 20, 10, 3]  # 2 hidden layers + 3-class output\n",
    "\n",
    "# Train model\n",
    "parameters, costs = l_layer_model(X_fake, Y_fake, layer_dims, learning_rate=0.05, num_iterations=500, use_batchnorm=False)\n",
    "\n",
    "# Predict\n",
    "acc = predict(X_fake, Y_fake, parameters)\n",
    "print(f\"\\n✅ Fake training accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "# Optional: plot cost\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.title(\"Cost over iterations (Sanity Check)\")\n",
    "plt.xlabel(\"Iteration (x100)\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    # Loop through layers to initialize weights and biases\n",
    "    for l in range(1, L):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the function initialize_parameters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 784)\n",
      "(20, 1)\n",
      "(10, 5)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [784, 20, 7, 5, 10]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "print(parameters[\"W1\"].shape)  # Should print (20, 784)\n",
    "print(parameters[\"b1\"].shape)  # Should print (20, 1)\n",
    "print(parameters[\"W4\"].shape)  # Should print (10, 5)\n",
    "print(parameters[\"b4\"].shape)  # Should print (10, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Compute the linear part of a layer's forward propagation.\n",
    "    \n",
    "    Arguments:\n",
    "    A -- activations from previous layer\n",
    "    W -- weights matrix of current layer\n",
    "    b -- bias vector of current layer\n",
    "    \n",
    "    Returns:\n",
    "    Z -- linear component (W.A + b)\n",
    "    linear_cache -- tuple (A, W, b) for backpropagation\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    linear_cache = (A, W, b)\n",
    "    return Z, linear_cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the function linear_forward**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z shape: (4, 2)\n",
      "Z: [[ 0.48338097  0.15237448]\n",
      " [-1.33390458 -2.08407482]\n",
      " [ 0.7505205  -1.0779219 ]\n",
      " [-1.23465366 -1.76418096]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "A_prev = np.random.randn(3, 2)\n",
    "W = np.random.randn(4, 3)\n",
    "b = np.random.randn(4, 1)\n",
    "\n",
    "Z, cache = linear_forward(A_prev, W, b)\n",
    "print(\"Z shape:\", Z.shape)  # Should be (4, 2)\n",
    "print(\"Z:\", Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Implements the softmax activation function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of shape (number of classes, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    A -- output of softmax(Z), same shape as Z\n",
    "    activation_cache -- returns Z for use in backpropagation\n",
    "    \"\"\"\n",
    "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)  # for numerical stability\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    \n",
    "    activation_cache = Z\n",
    "    return A, activation_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implements the ReLU activation function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- output of relu(Z), same shape as Z\n",
    "    activation_cache -- returns Z for use in backpropagation\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    activation_cache = Z\n",
    "    return A, activation_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU result:\n",
      " [[1 0 0]\n",
      " [2 0 3]]\n",
      "Softmax result:\n",
      " [[0.26894142 0.73105858 0.04742587]\n",
      " [0.73105858 0.26894142 0.95257413]]\n"
     ]
    }
   ],
   "source": [
    "Z = np.array([[1, -1, 0],\n",
    "              [2, -2, 3]])\n",
    "\n",
    "A_relu, cache_relu = relu(Z)\n",
    "A_softmax, cache_softmax = softmax(Z)\n",
    "\n",
    "print(\"ReLU result:\\n\", A_relu)\n",
    "print(\"Softmax result:\\n\", A_softmax)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
